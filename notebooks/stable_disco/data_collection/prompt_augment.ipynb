{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc2e080b-b0f1-4796-975a-9b786dcd9f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext tensorboard\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "sys.path.insert(0, os.path.abspath('../..'))\n",
    "\n",
    "import importlib\n",
    "\n",
    "import contextlib\n",
    "\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from fileutils import get_default_path\n",
    "\n",
    "from automation.automationmanager import make_default_manager\n",
    "from automation.crawler.midjourneycrawl import crawl_gallery_user, crawl_gallery_feed\n",
    "from automation.midjourney.midjourneyutils import FeedType\n",
    "\n",
    "from storage.data.image.remoteimageinfo import imgs_to_cmds\n",
    "from storage.data.image.crawledimagegroups import CrawledImageGroups\n",
    "from storage.data.command import Command\n",
    "from storage.data.command.commandbuilder import CommandBuilder\n",
    "from storage.data.user.userids import MJ_USER_TO_ID\n",
    "from storage.data.user.mjuser import MJUser\n",
    "import time\n",
    "from util import Stopwatch\n",
    "import datetime\n",
    "\n",
    "import ai.stabledisco as sd\n",
    "import ai.torchmodules as torchmodules\n",
    "import ai.torchmodules.data as torchdata\n",
    "import ai.torchmodules.utils as torchutils\n",
    "import ai.stabledisco.utils as sdutils\n",
    "import clip\n",
    "import ai.nlp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5749847b-12e2-4019-a743-faaaed5773ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_file_path = get_default_path(\"large_datasets\", \"aug_prompts.feather\")\n",
    "if \"prompt_dataframe\" in dir():\n",
    "    del prompt_dataframe\n",
    "prompt_dataframe = pd.read_feather(df_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8147d89-7932-40ad-8e8a-3dbadee979ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_file_path = get_default_path(\"large_datasets\", \"aug_prompts.feather\")\n",
    "prompt_dataframe.to_feather(df_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9100d2a-4aa7-4e2e-b50b-1e54c7526637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16798395\n"
     ]
    }
   ],
   "source": [
    "print(len(prompt_dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f65bcef-f243-4581-8572-d7fa5a36fe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import markovify\n",
    "sentences = []\n",
    "\n",
    "prompt_sample_num = 2000000\n",
    "\n",
    "to_gen = 2000000\n",
    "per_markov = to_gen//10\n",
    "iters = to_gen // per_markov\n",
    "for markov_num in range(iters):\n",
    "    prompts = list(prompt_dataframe.sample(n=prompt_sample_num)[\"prompt\"])\n",
    "    print(f\"Making markov {markov_num}\")\n",
    "    prompt_markov = markovify.NewlineText(prompts, 2, retain_original=False)\n",
    "    del prompts\n",
    "    prompt_markov.compile(inplace=True)\n",
    "    for idx in range(per_markov):\n",
    "        if idx % 2500 == 0:\n",
    "            print(idx)\n",
    "        sent = prompt_markov.make_sentence(test_output=False)\n",
    "        if sent:\n",
    "            sentences.append(sent)\n",
    "    del prompt_markov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b445657c-664e-42cd-807e-c3894eab26a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for sent in sentences:\n",
    "    if not sent:\n",
    "        continue\n",
    "    rows.append({\n",
    "        \"prompt\": sent.lower(),\n",
    "        \"text_tokens\": clip.tokenize(sent, truncate=True)[0].numpy().astype(np.uint16)  \n",
    "    })\n",
    "    \n",
    "prompt_dataframe = pd.concat([prompt_dataframe, pd.DataFrame.from_records(rows)], ignore_index=True)\n",
    "prompt_dataframe.drop_duplicates(subset=\"prompt\", ignore_index=True, inplace=True)\n",
    "prompt_dataframe = prompt_dataframe.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df_file_path = get_default_path(\"large_datasets\", \"aug_prompts.feather\")\n",
    "prompt_dataframe.to_feather(df_file_path)\n",
    "print(len(prompt_dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e44b507f-54e4-40b3-b3ec-6fc4ddd8e921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prompt         object\n",
       "text_tokens    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_dataframe.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12cb219e-7176-49e5-961d-d587a55e949a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in prompt_dataframe.itertuples(True):\n",
    "    idx = row[0]\n",
    "    if type(row.text_tokens[0]) == np.ndarray:\n",
    "        prompt_dataframe.at[idx, \"text_tokens\"] = row.text_tokens[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01c9d14-e2eb-446f-8736-518cc8a2c1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(sentences)\n",
    "print(\"\\n\\n\".join(sentences[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fe0a540-c06f-4c01-8099-649ec33d2be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "df_file_path = get_default_path(\"large_datasets\", \"aug_prompts.feather\")\n",
    "markov_file_path = get_default_path(\"large_datasets\", \"aug_markov.pk\")\n",
    "with open(markov_file_path, 'wb+') as outfile:\n",
    "    pickle.dump(prompt_markov, outfile)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79326913-1b73-4875-83b7-209a823dfdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "from clip.clip import _tokenizer as clip_tokenizer\n",
    "eot_token = clip_tokenizer.encoder[\"<|endoftext|>\"]\n",
    "\n",
    "vit14_clip_model, vit14_clip_preprocess = clip.load('ViT-L/14')\n",
    "vit14_clip_model = vit14_clip_model.float().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87b8a9ec-9bfc-43f3-a6d8-42529f70b666",
   "metadata": {},
   "outputs": [],
   "source": [
    "sot_token = clip_tokenizer.encoder[\"<|startoftext|>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "439f8cdc-2a0d-4447-9c40-ebb1a72dbdd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n"
     ]
    }
   ],
   "source": [
    "token_cnt = {token: 0 for token in clip_tokenizer.encoder.values()}\n",
    "eos_token = clip\n",
    "for idx, tokens in enumerate(prompt_dataframe[\"text_tokens\"]):\n",
    "    for val in tokens:\n",
    "        token_cnt[val] += 1\n",
    "        if val == eot_token:\n",
    "            break\n",
    "            \n",
    "    if idx % 100000 == 0:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5459e2e9-38db-4691-8de5-dd18c8449924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perc: 0 424.0\n",
      "perc: 5 441.0\n",
      "perc: 10 443.0\n",
      "perc: 15 445.0\n",
      "perc: 20 446.0\n",
      "perc: 25 447.0\n",
      "perc: 30 448.0\n",
      "perc: 35 449.0\n",
      "perc: 40 450.0\n",
      "perc: 45 452.0\n",
      "perc: 50 453.0\n",
      "perc: 55 455.0\n",
      "perc: 60 457.0\n",
      "perc: 65 461.0\n",
      "perc: 70 467.0\n",
      "perc: 75 477.0\n",
      "perc: 80 492.0\n",
      "perc: 85 524.0\n",
      "perc: 90 709.0\n",
      "perc: 95 1831.2999999999884\n",
      "perc: 100 11642068.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "cnt_token = [(val, key) for key, val in token_cnt.items()]\n",
    "cnts = [pair[0] for pair in cnt_token]\n",
    "cnt_token.sort()\n",
    "\n",
    "target_min = np.percentile(cnts, 90)\n",
    "replacement_min = np.percentile(cnts, 90)\n",
    "replacement_max = np.percentile(cnts, 99.9)\n",
    "\n",
    "for perc in range(0, 101, 5):\n",
    "    print(f\"perc: {perc}\", np.percentile(cnts, perc))\n",
    "\n",
    "to_add_cnts = [[target_min - val, key] for key, val in token_cnt.items() if val < target_min]\n",
    "replacement_candidates = {key  for key, val in token_cnt.items() if replacement_max > val > replacement_min}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2fbf853-a018-494c-8a42-ac3aff0d5422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 rem 46711\n",
      "100000 rem 46709\n",
      "200000 rem 46705\n",
      "300000 rem 46697\n",
      "400000 rem 46690\n",
      "500000 rem 46684\n",
      "600000 rem 46676\n",
      "700000 rem 46670\n",
      "800000 rem 46662\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m tokens \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(orig_tokens)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tokens[\u001b[38;5;241m1\u001b[39m:]):\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;241m==\u001b[39m \u001b[43meot_token\u001b[49m:\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m replacement_candidates \u001b[38;5;129;01mor\u001b[39;00m random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m<\u001b[39m keep_chance:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "import copy\n",
    "new_tokens = []\n",
    "\n",
    "keep_chance = 0.75\n",
    "for idx, orig_tokens in enumerate(prompt_dataframe[\"text_tokens\"]):\n",
    "    if idx % 100000 == 0:\n",
    "        print(idx, \"rem\", len(to_add_cnts))\n",
    "    replaced = 0\n",
    "    tokens = copy.copy(orig_tokens)\n",
    "    for idx, token in enumerate(tokens[1:]):\n",
    "        if token == eot_token:\n",
    "            break\n",
    "            \n",
    "        if token not in replacement_candidates or random.random() < keep_chance:\n",
    "            continue\n",
    "            \n",
    "        replaced += 1\n",
    "        rep_token_idx = random.randint(0, len(to_add_cnts)-1)\n",
    "        rep_token = to_add_cnts[rep_token_idx][1]\n",
    "        tokens[idx+1] = rep_token\n",
    "        \n",
    "        to_add_cnts[rep_token_idx][0] -= 1\n",
    "        if to_add_cnts[rep_token_idx][0] < 1:\n",
    "            del to_add_cnts[rep_token_idx]\n",
    "            \n",
    "        if len(to_add_cnts) < 1:\n",
    "            break\n",
    "\n",
    "    if replaced > 0:\n",
    "        new_tokens.append(tokens)\n",
    "\n",
    "    if len(to_add_cnts) < 1:\n",
    "        break\n",
    "print(len(new_tokens))\n",
    "new_tokens = np.array(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2df7ee8c-18ed-4199-aa57-b0e2fdf4f6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [sdutils.decode_clip_tokens(aug_tokens)[0] for aug_tokens in new_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c7710667-a5b3-4d95-92c2-c8fc95dedb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame.from_dict({\"prompt\": text, \"text_tokens\": new_tokens.tolist()})\n",
    "prompt_dataframe = pd.concat([prompt_dataframe, new_df], ignore_index=True) \n",
    "del new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5ba24120-594a-4cbc-abe3-8cf51ef75bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_dataframe = prompt_dataframe.sample(frac=1).reset_index(drop=True)\n",
    "prompt_dataframe.to_feather(df_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stable-disco",
   "language": "python",
   "name": "stable-disco"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
